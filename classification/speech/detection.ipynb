{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:51:09.274149Z",
     "start_time": "2024-11-02T23:51:03.170493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import librosa\n",
    "import time\n",
    "from IPython.display import clear_output"
   ],
   "id": "1249735fabd24ca",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-02T23:51:27.333503Z",
     "start_time": "2024-11-02T23:51:27.313213Z"
    }
   },
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_1(nn.Module):\n",
    "    def __init__(self, n=32):\n",
    "        super(CNN_1, self).__init__()\n",
    "        self.n = n\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, self.n, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout1d(p=0.3)\n",
    "        self.conv2 = nn.Conv1d(self.n, self.n // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout1d(p=0.3)\n",
    "        self.conv3 = nn.Conv1d(self.n // 2, self.n // 2, kernel_size=3, padding=1)\n",
    "        self.conv3_dropout = nn.Dropout1d(p=0.3)\n",
    "\n",
    "        # Calculate the correct input size for the fully connected layer\n",
    "        self.fc1 = nn.Linear((self.n // 2) * (197 // 8), 32)  # 170 // 8 due to three max_pool1d with kernel_size=2\n",
    "        self.fc2 = nn.Linear(32, 7)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool1d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool1d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = F.max_pool1d(torch.tanh(self.conv3(out)), 2)\n",
    "        out = self.conv3_dropout(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.sigmoid(self.fc2(out))\n",
    "        return out\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:51:27.843509Z",
     "start_time": "2024-11-02T23:51:27.820963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.dropout_conv1 = nn.Dropout2d(p=0.5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1)\n",
    "        self.dropout_conv2 = nn.Dropout2d(p=0.5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1)\n",
    "        self.dropout_conv3 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 2 * (44 // 2 // 2 // 2), 256)  # Adjust based on the output size from feature extractor\n",
    "        self.fc1_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc2_dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(128, 7)  # 7 output units for 7 emotions\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.dropout_conv1(torch.relu(self.conv1(x))))\n",
    "        x = self.pool(self.dropout_conv2(torch.relu(self.conv2(x))))\n",
    "        x = self.pool(self.dropout_conv3(torch.relu(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1_dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2_dropout(torch.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return self.sigmoid(x)"
   ],
   "id": "bcf3cad5f6286842",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:51:29.569658Z",
     "start_time": "2024-11-02T23:51:29.547156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa.feature\n",
    "import numpy as np\n",
    "\n",
    "def extract_zcr(audio):\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    zcr_stats = np.concatenate([np.mean(zcr.T, axis=0), np.std(zcr.T, axis=0)])\n",
    "    return zcr_stats\n",
    "\n",
    "\n",
    "def extract_chroma(audio, sr):\n",
    "    chroma = librosa.feature.chroma_stft(S=audio, sr=sr)\n",
    "    chroma_stats = np.concatenate([np.mean(chroma.T, axis=0), np.std(chroma.T, axis=0)])\n",
    "    return chroma_stats\n",
    "\n",
    "\n",
    "def extract_mfccs(audio, sr):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfcc_stats = np.concatenate([np.mean(mfccs.T, axis=0), np.std(mfccs.T, axis=0)])\n",
    "    return mfcc_stats\n",
    "\n",
    "\n",
    "def extract_spectral_contrast(audio, sr):\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "    spectral_contrast_stats = np.concatenate(\n",
    "        [np.mean(spectral_contrast.T, axis=0), np.std(spectral_contrast.T, axis=0)])\n",
    "    return spectral_contrast_stats\n",
    "\n",
    "\n",
    "def extract_spectral_rolloff(audio, sr):\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "    spectral_rolloff_stats = np.concatenate([np.mean(spectral_rolloff.T, axis=0), np.std(spectral_rolloff.T, axis=0)])\n",
    "    return spectral_rolloff_stats\n",
    "\n",
    "\n",
    "def extract_rmse(audio):\n",
    "    rmse = librosa.feature.rms(y=audio)\n",
    "    rmse_stats = np.concatenate([np.mean(rmse.T, axis=0), np.std(rmse.T, axis=0)])\n",
    "    return rmse_stats\n",
    "\n",
    "\n",
    "def extract_mel_spectrogram(audio, sr):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    mel_spectrogram_mean = np.mean(mel_spectrogram.T, axis=0)\n",
    "    mel_spectrogram_db = librosa.power_to_db(librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=16), ref=np.max)\n",
    "    return mel_spectrogram_db, mel_spectrogram_mean\n",
    "\n",
    "\n",
    "def extract_features(data, sample_rate):\n",
    "    result = np.array([])\n",
    "\n",
    "    # ZCR\n",
    "    zcr = extract_zcr(data)\n",
    "    result = np.hstack((result, zcr))\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = extract_chroma(stft, sample_rate)\n",
    "    result = np.hstack((result, chroma_stft))\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = extract_mfccs(data, sample_rate)\n",
    "    result = np.hstack((result, mfcc))\n",
    "\n",
    "    # Spectral_contrast\n",
    "    spectral_contrast = extract_spectral_contrast(data, sample_rate)\n",
    "    result = np.hstack((result, spectral_contrast))\n",
    "\n",
    "    # Spectral_rolloff\n",
    "    spectral_rolloff = extract_spectral_rolloff(data, sample_rate)\n",
    "    result = np.hstack((result, spectral_rolloff))\n",
    "\n",
    "    # RMS\n",
    "    rms = extract_rmse(data)\n",
    "    result = np.hstack((result, rms))\n",
    "\n",
    "    # Mel spectrogram\n",
    "    mel_spectrogram, mel_spectrogram_mean = extract_mel_spectrogram(data, sample_rate)\n",
    "    result = np.hstack((result, mel_spectrogram_mean))\n",
    "    return result, mel_spectrogram.flatten()\n",
    "\n",
    "\n",
    "def get_features(path):\n",
    "    data, sample_rate = librosa.load(path)\n",
    "\n",
    "    extracted_features, mel_spectrogram = extract_features(data, sample_rate)\n",
    "    result = np.array(extracted_features)\n",
    "    mel_spectrogram = np.array(mel_spectrogram)\n",
    "\n",
    "    return result, mel_spectrogram"
   ],
   "id": "63978353da956698",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:51:30.988482Z",
     "start_time": "2024-11-02T23:51:30.980749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def noise_reduction(audio, sr, noise_profile=None):\n",
    "    # Compute the STFT of the signal\n",
    "    stft = librosa.stft(audio)\n",
    "    magnitude, phase = librosa.magphase(stft)\n",
    "\n",
    "    # Estimate the noise profile if not provided\n",
    "    if noise_profile is None:\n",
    "        noise_profile = np.median(magnitude, axis=1, keepdims=True)\n",
    "\n",
    "    # Subtract the noise profile from the magnitude\n",
    "    magnitude_cleaned = np.maximum(magnitude - noise_profile, 0)\n",
    "\n",
    "    # Reconstruct the audio signal from the cleaned magnitude and original phase\n",
    "    stft_cleaned = magnitude_cleaned * phase\n",
    "    audio_cleaned = librosa.istft(stft_cleaned)\n",
    "\n",
    "    return audio_cleaned"
   ],
   "id": "aada4aaf65623f20",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:51:33.334520Z",
     "start_time": "2024-11-02T23:51:33.023925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the model\n",
    "model1 = CNN_1().to(device)\n",
    "model1.load_state_dict(torch.load('outputs/model_CNN_2_1.t'))\n",
    "model1.eval()\n",
    "model2 = CNN_2().to(device)\n",
    "model2.load_state_dict(torch.load('outputs/model_CNN_2_2.t'))\n",
    "model2.eval()"
   ],
   "id": "8a9c442f5a9a77df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_2(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dropout_conv1): Dropout2d(p=0.5, inplace=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dropout_conv2): Dropout2d(p=0.5, inplace=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dropout_conv3): Dropout2d(p=0.5, inplace=False)\n",
       "  (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=256, bias=True)\n",
       "  (fc1_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=128, out_features=7, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:52:45.600645Z",
     "start_time": "2024-11-02T23:51:38.196729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Audio configuration\n",
    "SAMPLE_RATE = 22050  # Hz\n",
    "DURATION = 1  # seconds\n",
    "FRAME_SIZE = int(SAMPLE_RATE * DURATION)\n",
    "CHANNELS = 1\n",
    "FORMAT = pyaudio.paInt16\n",
    "N_MELS = 16\n",
    "TIME_FRAMES = int(704 / N_MELS)\n",
    "emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprised\"]\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Callback function for real-time classification\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    # Convert bytes to numpy array\n",
    "    audio_data = np.frombuffer(in_data, dtype=np.int16).astype(np.float32)\n",
    "    audio_data = noise_reduction(audio_data, SAMPLE_RATE)\n",
    "\n",
    "    # Extract features\n",
    "    extracted_features, mel_spectrogram = extract_features(audio_data, SAMPLE_RATE)\n",
    "    \n",
    "    # Prepare input tensors\n",
    "    features_tensor = torch.tensor(extracted_features, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    mel_spectrogram_tensor = torch.tensor(mel_spectrogram.reshape(-1, 1, N_MELS, TIME_FRAMES), dtype=torch.float32)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        output_cnn1 = model1(features_tensor.to(device))\n",
    "        output_cnn2 = model2(mel_spectrogram_tensor.to(device))\n",
    "    \n",
    "    # Calculate the average probability for each class\n",
    "    avg_probabilities = (output_cnn1 + output_cnn2) / 2\n",
    "    avg_probabilities = avg_probabilities * 100\n",
    "    \n",
    "    avg_probabilities = avg_probabilities.flatten()\n",
    "    # avg_probabilities = [\n",
    "    #     avg_probabilities[0] - ,\n",
    "    #                      \n",
    "    #                      ]\n",
    "    max_index = np.argmax(avg_probabilities.to(\"cpu\"))\n",
    "    emotion = emotions[max_index]\n",
    "    # Print the probabilities and their average\n",
    "    # print(f\"CNN_1 Probabilities: {output_cnn1.numpy().flatten()}\")\n",
    "    # print(f\"CNN_2 Probabilities: {output_cnn2.numpy().flatten()}\")\n",
    "    # print(f\"Average Probabilities: {avg_probabilities.numpy().flatten()}\")\n",
    "\n",
    "    output = (\"Predicted Probabilities: \" \n",
    "          + \"{:>7.2f}%\".format(avg_probabilities.flatten()[0])\n",
    "          + \"{:>7.2f}%\".format(avg_probabilities.flatten()[1])\n",
    "          + \"{:>7.2f}%\".format(avg_probabilities.flatten()[2])\n",
    "          + \"{:>7.2f}%\".format(avg_probabilities.flatten()[3])\n",
    "          + \"{:>7.2f}%\".format(avg_probabilities.flatten()[4])\n",
    "          + \"{:>7.2f}%\".format(avg_probabilities.flatten()[5])\n",
    "          + \"{:>7.2f}%\".format(avg_probabilities.flatten()[6])\n",
    "          + f\"  {emotion}\")\n",
    "    print(output.ljust(100), end=\"\\r\")\n",
    "\n",
    "    return (in_data, pyaudio.paContinue)\n",
    "\n",
    "# Open the audio stream\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=SAMPLE_RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=FRAME_SIZE,\n",
    "                stream_callback=callback)\n",
    "\n",
    "# Start the stream\n",
    "print(\"Listening... Press Ctrl+C to stop.\")\n",
    "print(\"                         \", \n",
    "      \"angry   \",\n",
    "      \"disgust \",\n",
    "      \"fear  \",\n",
    "      \"happy  \",\n",
    "      \"neutral \",\n",
    "      \"sad  \",\n",
    "      \"surprised\",)\n",
    "stream.start_stream()\n",
    "\n",
    "try:\n",
    "    while stream.is_active():\n",
    "        pass\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped.\")\n",
    "finally:\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "# 5, 48, 25, 28,  \n"
   ],
   "id": "dd30ea745928e173",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening... Press Ctrl+C to stop.\n",
      "                          angry    disgust  fear   happy   neutral  sad   surprised\n",
      "Stopped.d Probabilities:    4.99%  53.48%  43.14%  33.14%  42.15%  61.04%   6.28%  sad              \n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
