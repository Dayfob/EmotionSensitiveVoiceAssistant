{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9d079a99918ff5",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "Author: Alikhan Semembayev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cf150365de97",
   "metadata": {},
   "source": [
    "## 1. Perform necessary data preprocessing, e.g. removing punctuation and stop words, stemming, lemmatizing. You may use the outputs from previous weekly assignments. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f03f9e57e3e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import demoji\n",
    "import svgling\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from autocorrect import Speller\n",
    "import re\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "email_re = r\"\\b[A-Za-z]+@\\S+\\b\"\n",
    "ssn_re = r\"\\b[0-9]{3}-[0-9]{2}-[0-9]{4}\\b\"\n",
    "ip_re = r\"\\b\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}\\b\"\n",
    "\n",
    "street_number_re = r\"^\\d{1,}\"\n",
    "street_name_re = r\"[a-zA-Z0-9\\s]+,?\"\n",
    "city_name_re = r\" [a-zA-Z]+(\\,)?\"\n",
    "state_abbrev_re = r\" [A-Z]{2}\"\n",
    "postal_code_re = r\" [0-9]{5}$\"\n",
    "address_pattern_re = r\"\" + street_number_re + street_name_re + city_name_re + state_abbrev_re + postal_code_re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace emojis\n",
    "    text = demoji.replace(text)\n",
    "\n",
    "    # Remove smart quotes and dashes\n",
    "    text = text.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    # print(words)\n",
    "\n",
    "    # Spelling correction + replace all t with not\n",
    "    words = ['not' if word == 't' else (\n",
    "        'ADDRESS' if re.match(address_pattern_re, word)\n",
    "        else (\n",
    "            'EMAIL' if re.match(email_re, word)\n",
    "            else (\n",
    "                'SSN' if re.match(ssn_re, word)\n",
    "                else (\n",
    "                    'IP' if re.match(ip_re, word)\n",
    "                    else spell(word)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ) for word in words]\n",
    "\n",
    "    # Remove stop words and non-alphabetic tokens and punctuation\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words or word in ['not', 'no']]\n",
    "\n",
    "    # POS tagging and Lemmatization\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    tag_map = defaultdict(lambda: \"n\")\n",
    "    tag_map[\"N\"] = \"n\"\n",
    "    tag_map[\"V\"] = \"v\"\n",
    "    tag_map[\"J\"] = \"a\"\n",
    "    tag_map[\"R\"] = \"r\"\n",
    "\n",
    "    words = [lemmatizer.lemmatize(word, pos=tag_map[tag[0]]) for word, tag in tagged_words]\n",
    "\n",
    "    # Return cleaned words as a single string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed44700a959b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('../../../../data/text/combined_raw.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "for row in data.values:\n",
    "    row[0] = clean_text(row[0])\n",
    "\n",
    "data.to_csv('../../../../data/text/combined_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "324008f6c981c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T04:51:28.248397Z",
     "start_time": "2024-11-07T04:51:28.065287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    emotion\n",
      "0  freshwater fish drink water skin via osmosis s...      happy\n",
      "1  think everyone must use daily become grained e...    neutral\n",
      "2  agree google headquarters mountain view califo...    neutral\n",
      "3  thats funny current ceo sunday ficha didnt kno...    neutral\n",
      "4  oh yeah not know either also want go google al...  surprised\n",
      "5                                                say  surprised\n",
      "6        yeah apparently lol instead hire people row      happy\n",
      "7  thats funny guess imaginative leave huge tech ...  surprised\n",
      "8  yeah exactly sure cheap one thing bet not expl...  surprised\n",
      "9  remember hearing immortality waste jellyfish h...    neutral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('../../../../data/text/combined_cleaned.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f03df5b276533",
   "metadata": {},
   "source": [
    "## 2. For the binary classification problem you came up previously, build your own model by combining BERT with a classifier.  (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75351f59c3350559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T04:51:54.858078Z",
     "start_time": "2024-11-07T04:51:54.774367Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the maximum rows per label\n",
    "max_rows_per_label = 10000\n",
    "\n",
    "# Sample rows for each label\n",
    "balanced_data = data.groupby('emotion', group_keys=False).apply(lambda x: x.sample(n=min(len(x), max_rows_per_label)))\n",
    "\n",
    "# Save or use the balanced data\n",
    "balanced_data.to_csv(\"../../../../data/text/combined_cleaned_balanced_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b93ed7f4a0cc11a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:06:43.631901Z",
     "start_time": "2024-11-07T19:06:42.972989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  is_happy  is_surprised  \\\n",
      "0  freshwater fish drink water skin via osmosis s...         1             0   \n",
      "1  think everyone must use daily become grained e...         0             0   \n",
      "2  agree google headquarters mountain view califo...         0             0   \n",
      "3  thats funny current ceo sunday ficha didnt kno...         0             0   \n",
      "4  oh yeah not know either also want go google al...         0             1   \n",
      "5                                                say         0             1   \n",
      "6        yeah apparently lol instead hire people row         1             0   \n",
      "7  thats funny guess imaginative leave huge tech ...         0             1   \n",
      "8  yeah exactly sure cheap one thing bet not expl...         0             1   \n",
      "9  remember hearing immortality waste jellyfish h...         0             0   \n",
      "\n",
      "   is_neutral  is_sad  is_fear  is_angry  is_disgust  \n",
      "0           0       0        0         0           0  \n",
      "1           1       0        0         0           0  \n",
      "2           1       0        0         0           0  \n",
      "3           1       0        0         0           0  \n",
      "4           0       0        0         0           0  \n",
      "5           0       0        0         0           0  \n",
      "6           0       0        0         0           0  \n",
      "7           0       0        0         0           0  \n",
      "8           0       0        0         0           0  \n",
      "9           1       0        0         0           0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('../../../../data/text/combined_cleaned_multilabel.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a003f153111a09b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:22:49.774133Z",
     "start_time": "2024-11-07T19:22:49.761367Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from transformers import BertModel\n",
    "\n",
    "# Custom Dataset Class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.texts[idx],\n",
    "            # 'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "        \n",
    "        # Freeze BERT parameters\n",
    "        for param in self.bert.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c64d97da267b64d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:23:11.367523Z",
     "start_time": "2024-11-07T19:22:50.838239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = data['is_happy']\n",
    "num_classes = 2\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 50\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encodings['input_ids'].tolist()\n",
    "\n",
    "# Convert texts to token IDs\n",
    "data['input_ids'] = tokenize_texts(data['text'])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['input_ids'].tolist(), data['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4229f5c403484a4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:23:11.378956Z",
     "start_time": "2024-11-07T19:23:11.367523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset = EmotionDataset(X_train, y_train)\n",
    "test_dataset = EmotionDataset(X_test, y_test)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a7bb376a1f65d",
   "metadata": {},
   "source": [
    "## 3. Train your own model by fine-tuning BERT. And save your model and use it to classify sentences (50 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dde424710eb7720c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:26:32.588402Z",
     "start_time": "2024-11-07T19:23:24.160316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/922: Loss = 0.5630530714988708\n",
      "Batch 20/922: Loss = 0.6131772994995117\n",
      "Batch 30/922: Loss = 0.6327952146530151\n",
      "Batch 40/922: Loss = 0.5326545834541321\n",
      "Batch 50/922: Loss = 0.6016024947166443\n",
      "Batch 60/922: Loss = 0.6733378767967224\n",
      "Batch 70/922: Loss = 0.5222554802894592\n",
      "Batch 80/922: Loss = 0.6361863613128662\n",
      "Batch 90/922: Loss = 0.6165667772293091\n",
      "Batch 100/922: Loss = 0.6207923889160156\n",
      "Batch 110/922: Loss = 0.6651461124420166\n",
      "Batch 120/922: Loss = 0.5742948651313782\n",
      "Batch 130/922: Loss = 0.607204258441925\n",
      "Batch 140/922: Loss = 0.6520662903785706\n",
      "Batch 150/922: Loss = 0.6315717697143555\n",
      "Batch 160/922: Loss = 0.5946286916732788\n",
      "Batch 170/922: Loss = 0.6228736639022827\n",
      "Batch 180/922: Loss = 0.6310967206954956\n",
      "Batch 190/922: Loss = 0.6357423067092896\n",
      "Batch 200/922: Loss = 0.6597950458526611\n",
      "Batch 210/922: Loss = 0.6262190341949463\n",
      "Batch 220/922: Loss = 0.6458306312561035\n",
      "Batch 230/922: Loss = 0.5219413042068481\n",
      "Batch 240/922: Loss = 0.5974372625350952\n",
      "Batch 250/922: Loss = 0.5697453022003174\n",
      "Batch 260/922: Loss = 0.56712406873703\n",
      "Batch 270/922: Loss = 0.5751973390579224\n",
      "Batch 280/922: Loss = 0.5681144595146179\n",
      "Batch 290/922: Loss = 0.6196956634521484\n",
      "Batch 300/922: Loss = 0.5714631676673889\n",
      "Batch 310/922: Loss = 0.6176434755325317\n",
      "Batch 320/922: Loss = 0.5966416001319885\n",
      "Batch 330/922: Loss = 0.6372080445289612\n",
      "Batch 340/922: Loss = 0.6497365832328796\n",
      "Batch 350/922: Loss = 0.6117145419120789\n",
      "Batch 360/922: Loss = 0.6185929775238037\n",
      "Batch 370/922: Loss = 0.579983115196228\n",
      "Batch 380/922: Loss = 0.6225438714027405\n",
      "Batch 390/922: Loss = 0.6509799957275391\n",
      "Batch 400/922: Loss = 0.6598306894302368\n",
      "Batch 410/922: Loss = 0.665298342704773\n",
      "Batch 420/922: Loss = 0.6188740134239197\n",
      "Batch 430/922: Loss = 0.5753017663955688\n",
      "Batch 440/922: Loss = 0.5918484926223755\n",
      "Batch 450/922: Loss = 0.5168279409408569\n",
      "Batch 460/922: Loss = 0.6281508207321167\n",
      "Batch 470/922: Loss = 0.550732433795929\n",
      "Batch 480/922: Loss = 0.6071640849113464\n",
      "Batch 490/922: Loss = 0.5696375370025635\n",
      "Batch 500/922: Loss = 0.5995907783508301\n",
      "Batch 510/922: Loss = 0.5999315977096558\n",
      "Batch 520/922: Loss = 0.6529277563095093\n",
      "Batch 530/922: Loss = 0.656372606754303\n",
      "Batch 540/922: Loss = 0.6001768112182617\n",
      "Batch 550/922: Loss = 0.5888123512268066\n",
      "Batch 560/922: Loss = 0.5740253329277039\n",
      "Batch 570/922: Loss = 0.5580939650535583\n",
      "Batch 580/922: Loss = 0.6496306657791138\n",
      "Batch 590/922: Loss = 0.5807874202728271\n",
      "Batch 600/922: Loss = 0.5930541157722473\n",
      "Batch 610/922: Loss = 0.6509097218513489\n",
      "Batch 620/922: Loss = 0.7108039259910583\n",
      "Batch 630/922: Loss = 0.6772646307945251\n",
      "Batch 640/922: Loss = 0.6549835801124573\n",
      "Batch 650/922: Loss = 0.7101112604141235\n",
      "Batch 660/922: Loss = 0.6099209785461426\n",
      "Batch 670/922: Loss = 0.6350319981575012\n",
      "Batch 680/922: Loss = 0.5944700241088867\n",
      "Batch 690/922: Loss = 0.5819827318191528\n",
      "Batch 700/922: Loss = 0.5864325761795044\n",
      "Batch 710/922: Loss = 0.6136187314987183\n",
      "Batch 720/922: Loss = 0.620949923992157\n",
      "Batch 730/922: Loss = 0.5451276302337646\n",
      "Batch 740/922: Loss = 0.6207963228225708\n",
      "Batch 750/922: Loss = 0.5374840497970581\n",
      "Batch 760/922: Loss = 0.5686373710632324\n",
      "Batch 770/922: Loss = 0.5706128478050232\n",
      "Batch 780/922: Loss = 0.614002525806427\n",
      "Batch 790/922: Loss = 0.5065705180168152\n",
      "Batch 800/922: Loss = 0.561711847782135\n",
      "Batch 810/922: Loss = 0.6020426750183105\n",
      "Batch 820/922: Loss = 0.6365953683853149\n",
      "Batch 830/922: Loss = 0.6307429075241089\n",
      "Batch 840/922: Loss = 0.5569747686386108\n",
      "Batch 850/922: Loss = 0.6081017851829529\n",
      "Batch 860/922: Loss = 0.5835798978805542\n",
      "Batch 870/922: Loss = 0.5943648815155029\n",
      "Batch 880/922: Loss = 0.6243953108787537\n",
      "Batch 890/922: Loss = 0.6308507919311523\n",
      "Batch 900/922: Loss = 0.5880237817764282\n",
      "Batch 910/922: Loss = 0.6128373146057129\n",
      "Batch 920/922: Loss = 0.6365931034088135\n",
      "Batch 922/922: Loss = 0.5860567092895508\n",
      "Epoch 1/2, Loss: 0.5860567092895508\n",
      "Batch 10/922: Loss = 0.6269892454147339\n",
      "Batch 20/922: Loss = 0.6112962365150452\n",
      "Batch 30/922: Loss = 0.5761814117431641\n",
      "Batch 40/922: Loss = 0.605475664138794\n",
      "Batch 50/922: Loss = 0.6240124106407166\n",
      "Batch 60/922: Loss = 0.5338467359542847\n",
      "Batch 70/922: Loss = 0.5576759576797485\n",
      "Batch 80/922: Loss = 0.575230598449707\n",
      "Batch 90/922: Loss = 0.594041645526886\n",
      "Batch 100/922: Loss = 0.5847200155258179\n",
      "Batch 110/922: Loss = 0.6131572723388672\n",
      "Batch 120/922: Loss = 0.5728012919425964\n",
      "Batch 130/922: Loss = 0.6537070870399475\n",
      "Batch 140/922: Loss = 0.5722397565841675\n",
      "Batch 150/922: Loss = 0.5651401281356812\n",
      "Batch 160/922: Loss = 0.5597229599952698\n",
      "Batch 170/922: Loss = 0.6052383184432983\n",
      "Batch 180/922: Loss = 0.5780014395713806\n",
      "Batch 190/922: Loss = 0.5872494578361511\n",
      "Batch 200/922: Loss = 0.5713818669319153\n",
      "Batch 210/922: Loss = 0.6001211404800415\n",
      "Batch 220/922: Loss = 0.5609968900680542\n",
      "Batch 230/922: Loss = 0.639348566532135\n",
      "Batch 240/922: Loss = 0.565445601940155\n",
      "Batch 250/922: Loss = 0.5864381790161133\n",
      "Batch 260/922: Loss = 0.6099804043769836\n",
      "Batch 270/922: Loss = 0.6042872667312622\n",
      "Batch 280/922: Loss = 0.5995222926139832\n",
      "Batch 290/922: Loss = 0.5948017239570618\n",
      "Batch 300/922: Loss = 0.6525886058807373\n",
      "Batch 310/922: Loss = 0.6476280093193054\n",
      "Batch 320/922: Loss = 0.5826666355133057\n",
      "Batch 330/922: Loss = 0.5883219242095947\n",
      "Batch 340/922: Loss = 0.5911024212837219\n",
      "Batch 350/922: Loss = 0.5833261609077454\n",
      "Batch 360/922: Loss = 0.6494131088256836\n",
      "Batch 370/922: Loss = 0.5831775069236755\n",
      "Batch 380/922: Loss = 0.6331817507743835\n",
      "Batch 390/922: Loss = 0.563279390335083\n",
      "Batch 400/922: Loss = 0.5506247282028198\n",
      "Batch 410/922: Loss = 0.6066303849220276\n",
      "Batch 420/922: Loss = 0.6082770824432373\n",
      "Batch 430/922: Loss = 0.6092134714126587\n",
      "Batch 440/922: Loss = 0.613429844379425\n",
      "Batch 450/922: Loss = 0.5280771255493164\n",
      "Batch 460/922: Loss = 0.6420609951019287\n",
      "Batch 470/922: Loss = 0.5771382451057434\n",
      "Batch 480/922: Loss = 0.5836032629013062\n",
      "Batch 490/922: Loss = 0.621660590171814\n",
      "Batch 500/922: Loss = 0.5207904577255249\n",
      "Batch 510/922: Loss = 0.5878470540046692\n",
      "Batch 520/922: Loss = 0.5718967318534851\n",
      "Batch 530/922: Loss = 0.6531387567520142\n",
      "Batch 540/922: Loss = 0.6168565154075623\n",
      "Batch 550/922: Loss = 0.592318058013916\n",
      "Batch 560/922: Loss = 0.586536169052124\n",
      "Batch 570/922: Loss = 0.592185378074646\n",
      "Batch 580/922: Loss = 0.6248476505279541\n",
      "Batch 590/922: Loss = 0.594023585319519\n",
      "Batch 600/922: Loss = 0.6537906527519226\n",
      "Batch 610/922: Loss = 0.5451487898826599\n",
      "Batch 620/922: Loss = 0.5406287908554077\n",
      "Batch 630/922: Loss = 0.5762225985527039\n",
      "Batch 640/922: Loss = 0.5909875631332397\n",
      "Batch 650/922: Loss = 0.6116880774497986\n",
      "Batch 660/922: Loss = 0.5797199010848999\n",
      "Batch 670/922: Loss = 0.6687448024749756\n",
      "Batch 680/922: Loss = 0.559087872505188\n",
      "Batch 690/922: Loss = 0.6240326166152954\n",
      "Batch 700/922: Loss = 0.5507158041000366\n",
      "Batch 710/922: Loss = 0.5524781346321106\n",
      "Batch 720/922: Loss = 0.5526093244552612\n",
      "Batch 730/922: Loss = 0.6186273097991943\n",
      "Batch 740/922: Loss = 0.5482545495033264\n",
      "Batch 750/922: Loss = 0.5862537622451782\n",
      "Batch 760/922: Loss = 0.5555471181869507\n",
      "Batch 770/922: Loss = 0.602100670337677\n",
      "Batch 780/922: Loss = 0.5909079313278198\n",
      "Batch 790/922: Loss = 0.6019454002380371\n",
      "Batch 800/922: Loss = 0.5035797953605652\n",
      "Batch 810/922: Loss = 0.5216773152351379\n",
      "Batch 820/922: Loss = 0.5819045901298523\n",
      "Batch 830/922: Loss = 0.5503303408622742\n",
      "Batch 840/922: Loss = 0.6687129139900208\n",
      "Batch 850/922: Loss = 0.5829569101333618\n",
      "Batch 860/922: Loss = 0.5864537358283997\n",
      "Batch 870/922: Loss = 0.5838385820388794\n",
      "Batch 880/922: Loss = 0.45380404591560364\n",
      "Batch 890/922: Loss = 0.6343964338302612\n",
      "Batch 900/922: Loss = 0.6097432971000671\n",
      "Batch 910/922: Loss = 0.5481880307197571\n",
      "Batch 920/922: Loss = 0.5437068939208984\n",
      "Batch 922/922: Loss = 0.6516422033309937\n",
      "Epoch 2/2, Loss: 0.6516422033309937\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = EmotionClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_loader, start=1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0 or batch_idx == len(train_loader):\n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)}: Loss = {loss.item()}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "836cc95b1963a66f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:27:24.315801Z",
     "start_time": "2024-11-07T19:27:24.271649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "Accuracy: 0.7088139503324739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not happy       0.71      1.00      0.83     20896\n",
      "       Happy       0.46      0.00      0.00      8580\n",
      "\n",
      "    accuracy                           0.71     29476\n",
      "   macro avg       0.58      0.50      0.42     29476\n",
      "weighted avg       0.64      0.71      0.59     29476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Move predictions and labels back to CPU for evaluation\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# # Classification Report\n",
    "# print(\"Classification Report:\")\n",
    "# # print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_, zero_division=\"warn\"))\n",
    "# print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds, target_names=['Not happy', 'Happy']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fdd31fb72fe5fbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:36:49.210637Z",
     "start_time": "2024-11-07T19:36:48.520059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a path to save the model\n",
    "model_save_path = \"emotion_classifier_model.pth\"\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b23f3b7093c70c9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:37:18.087989Z",
     "start_time": "2024-11-07T19:37:17.280214Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmotionClassifier(\n",
       "  (bert): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a new model instance\n",
    "loaded_model = EmotionClassifier(num_classes).to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode if you’re planning to evaluate or make predictions\n",
    "loaded_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d6b8215a2f7dc853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:38:30.200331Z",
     "start_time": "2024-11-07T19:38:10.375211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "Accuracy: 0.7088139503324739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not happy       0.71      1.00      0.83     20896\n",
      "       Happy       0.46      0.00      0.00      8580\n",
      "\n",
      "    accuracy                           0.71     29476\n",
      "   macro avg       0.58      0.50      0.42     29476\n",
      "weighted avg       0.64      0.71      0.59     29476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Move predictions and labels back to CPU for evaluation\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds, target_names=['Not happy', 'Happy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3a9709b19d7a0",
   "metadata": {},
   "source": [
    "## 4. Summarize what you have learned and discovered from Task 1-3. (10 points)\n",
    "\n",
    "1. Preprocessing is very important stage of development. Using stemming and lemmatization, along with removing stop words, helps improve text data representation. It decreases the size of dataset by removing unnecessary information, and optimizes it for training.\n",
    "2. Bart can be used as a layer of MLP. It has its own dropout. But we added another dropout layer. Also, we removed all other fully connected layers.\n",
    "3. We have very big amount of data. To save time we used high learning rate and small number of epochs for training. THe accuracy of the model predictions is 71%. \n",
    "4. Binary classification has better accuracy, because of fewer labels(possible outputs). Multicalss problem is more difficult and requires more time.\n",
    "5. It is possible to save the model with Bert as one of layers and load it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea801ff0671c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
