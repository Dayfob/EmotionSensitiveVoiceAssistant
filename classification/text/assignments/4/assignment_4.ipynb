{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Representations\n",
    "Author: Alikhan Semembayev"
   ],
   "id": "7b02acbebf12ba9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T22:37:28.568114Z",
     "start_time": "2024-10-08T22:37:28.344166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "print(movie_reviews.words())"
   ],
   "id": "8f05ab09de38496e",
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mmovie_reviews\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('movie_reviews')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/movie_reviews\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\semem/nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 84\u001B[0m     root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mzip_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    578\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 579\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mmovie_reviews\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('movie_reviews')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/movie_reviews.zip/movie_reviews/\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\semem/nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m movie_reviews\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmovie_reviews\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m())\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLazyCorpusLoader object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 120\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001B[39;00m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# __class__ to something new:\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     84\u001B[0m             root \u001B[38;5;241m=\u001B[39m nltk\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubdir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzip_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     85\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n\u001B[1;32m---> 86\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Load the corpus.\u001B[39;00m\n\u001B[0;32m     89\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__reader_cls(root, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__kwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 81\u001B[0m         root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     83\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    577\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m    578\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 579\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mmovie_reviews\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('movie_reviews')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/movie_reviews\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\semem/nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\semem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Perform necessary data preprocessing, e.g. removing punctuation and stop words, stemming, lemmatizing. You may use the outputs from previous weekly assignments. (10 points)",
   "id": "52486a8b8b66980e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import demoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace emojis\n",
    "    text = demoji.replace(text)\n",
    "    \n",
    "    # Remove smart quotes and dashes\n",
    "    text = text.replace(\"“\", \"\\\"\").replace(\"”\",\"\\\"\").replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    # print(words)\n",
    "    \n",
    "    # Spelling correction + replace all t with not\n",
    "    words = ['not' if word == 't' else spell(word) for word in words]\n",
    "\n",
    "    # Remove stop words and non-alphabetic tokens and punctuation\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words or word in ['not', 'no']]\n",
    "\n",
    "    # POS tagging and Lemmatization\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    tag_map = defaultdict(lambda: \"n\")\n",
    "    tag_map[\"N\"] = \"n\"\n",
    "    tag_map[\"V\"] = \"v\"\n",
    "    tag_map[\"J\"] = \"a\"\n",
    "    tag_map[\"R\"] = \"r\"\n",
    "    \n",
    "    words = [lemmatizer.lemmatize(word, pos=tag_map[tag[0]]) for word, tag in tagged_words]\n",
    "\n",
    "    # Return cleaned words as a single string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('../../data/text/combined_raw.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "for row in data.values:\n",
    "    row[0] = clean_text(row[0])\n",
    "\n",
    "data.to_csv('../../data/text/combined_cleaned.csv', index=False)\n"
   ],
   "id": "ff228db15ccb0d1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Count BoW on pre-processed data.  (10 points)",
   "id": "dd6f19f6b5d8db59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T20:36:06.821100Z",
     "start_time": "2024-10-08T20:36:06.545177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "data = (pd.read_csv('../../../../data/text/combined_cleaned.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "def count_bow(texts):\n",
    "    bow = Counter()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        bow.update(words)\n",
    "    return bow\n",
    "\n",
    "# Apply BoW counting\n",
    "bow_counts = count_bow(data['text'])\n",
    "\n",
    "# Convert the BoW count to a DataFrame\n",
    "bow_df = pd.DataFrame(list(bow_counts.items()), columns=['Word', 'Count'])\n",
    "\n",
    "# Save the result to a CSV file\n",
    "bow_df.to_csv('bow_output.csv', index=False)\n",
    "\n",
    "print(bow_df.head())\n"
   ],
   "id": "4e38b5d68f8f3c31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word  Count\n",
      "0  freshwater     40\n",
      "1        fish    605\n",
      "2       drink    379\n",
      "3       water    970\n",
      "4        skin    124\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:46:24.477730Z",
     "start_time": "2024-10-10T18:46:24.431952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('bow_output.csv'))\n",
    "\n",
    "data.sort_values(\"Count\", ascending=False, inplace=True)\n",
    "data = data.head(10)\n",
    "data.to_csv('bow_output_10.csv', index=False)"
   ],
   "id": "ab1c7420d7decc33",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:46:28.847642Z",
     "start_time": "2024-10-10T18:46:26.844265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "data = (pd.read_csv('bow_output_10.csv'))\n",
    "main = (pd.read_csv('../../../../data/text/combined_cleaned.csv'))\n",
    "main = main.dropna(how='any')\n",
    "main = main.head(1000)\n",
    "\n",
    "X = [] \n",
    "for row in main['text']: \n",
    "    vector = []\n",
    "    for word in data[\"Word\"]: \n",
    "        if word in nltk.word_tokenize(row): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X)"
   ],
   "id": "26eca627c7584f33",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:46:29.984356Z",
     "start_time": "2024-10-10T18:46:29.967996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert X to a pandas DataFrame to save as CSV\n",
    "X_df = pd.DataFrame(X, columns=data[\"Word\"])\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "X_df.to_csv('bow_vectors.csv', index=False)\n",
    "\n",
    "print(\"BoW vectors saved to 'bow_vectors.csv'.\")"
   ],
   "id": "35bc42cbd715d23a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW vectors saved to 'bow_vectors.csv'.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Compute TF-IDF vectors on pre-processed data.  (20 points)\n",
    "\n",
    "Due to the very large number of unique words, the output of the result has been changed. The second column of the table contains a sheet with TF-IDF vectors."
   ],
   "id": "ac07fb10bbdac843"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:49:37.554159Z",
     "start_time": "2024-10-10T18:49:37.544358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Term Frequency (TF)\n",
    "def compute_tf(text):\n",
    "    word_counts = Counter(text.split())\n",
    "    total_words = len(text.split())\n",
    "    tf = {word: count / total_words for word, count in word_counts.items()}\n",
    "    return tf\n",
    "\n",
    "# Inverse Document Frequency (IDF)\n",
    "def compute_idf(texts):\n",
    "    N = len(texts)\n",
    "    idf = {}\n",
    "    for text in texts:\n",
    "        for word in set(text.split()):\n",
    "            idf[word] = idf.get(word, 0) + 1\n",
    "    for word, doc_count in idf.items():\n",
    "        idf[word] = math.log(N / (doc_count + 1))  # Add 1 to avoid division by zero\n",
    "    return idf\n",
    "\n",
    "# Compute TF-IDF for each document\n",
    "def compute_tfidf(texts):\n",
    "    idf = compute_idf(texts)\n",
    "    tfidf = []\n",
    "    for idx, text in enumerate(texts):\n",
    "        tf = compute_tf(text)\n",
    "        tfidf_doc = [(word, tf[word] * idf[word]) for word in tf]  # Store word and its TF-IDF score as tuple\n",
    "        tfidf.append({'Document': idx + 1, 'TF-IDF': tfidf_doc})  # Use index as document ID\n",
    "    return tfidf\n",
    "\n",
    "# Apply TF-IDF computation to the clean text\n",
    "tfidf_alternative = compute_tfidf(data['text'])\n"
   ],
   "id": "ad2ff4a5aa75c65f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T22:17:57.545943Z",
     "start_time": "2024-10-08T22:17:55.004480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert TF-IDF results to a DataFrame for easier inspection\n",
    "# Each document will be a row, and the TF-IDF scores will be a list of (word, score) tuples\n",
    "tfidf_df_alternative = pd.DataFrame(tfidf_alternative)\n",
    "\n",
    "# Save the revised TF-IDF table to a CSV file\n",
    "tfidf_df_alternative.to_csv('tfidf_output.csv', index=False)\n",
    "\n",
    "# Print the first few rows of the TF-IDF table\n",
    "print(tfidf_df_alternative.head())"
   ],
   "id": "8fc5b4a1c14879cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Document                                             TF-IDF\n",
      "0         1  [(freshwater, 0.5148123472156599), (fish, 0.71...\n",
      "1         2  [(think, 0.23834696944350164), (everyone, 0.51...\n",
      "2         3  [(agree, 0.43660816089717763), (google, 1.0786...\n",
      "3         4  [(thats, 0.42304351063585904), (funny, 0.36702...\n",
      "4         5  [(oh, 0.2626823714954244), (yeah, 0.2042693288...\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Perform integer encoding and one-hot encoding on one of the pre-processed data files and save the output to a txt file.  (30 points)",
   "id": "8f33405a51d33e88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T19:42:10.810362Z",
     "start_time": "2024-10-08T19:42:10.495044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Integer encoding of the emotion labels\n",
    "data['emotion_encoded'] = label_encoder.fit_transform(data['emotion'])\n",
    "\n",
    "# Initialize one-hot encoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-hot encoding of the integer encoded labels\n",
    "emotion_onehot = onehot_encoder.fit_transform(data[['emotion_encoded']])\n",
    "\n",
    "# Save integer encoded and one-hot encoded labels to a file\n",
    "encoded_df = pd.DataFrame(emotion_onehot, columns=label_encoder.classes_)\n",
    "encoded_df.to_csv('emotion_onehot_encoded.txt', index=False)\n",
    "\n",
    "# Save integer encoding as well\n",
    "data[['emotion', 'emotion_encoded']].to_csv('emotion_integer_encoded.txt', index=False)\n",
    "\n",
    "# Display encoded data\n",
    "print(data[['emotion', 'emotion_encoded']].head())\n",
    "print(encoded_df.head())\n"
   ],
   "id": "335405fe11a8b05f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emotion  emotion_encoded\n",
      "0      happy                3\n",
      "1    neutral                4\n",
      "2    neutral                4\n",
      "3    neutral                4\n",
      "4  surprised                6\n",
      "   angry  disgust  fear  happy  neutral  sad  surprised\n",
      "0    0.0      0.0   0.0    1.0      0.0  0.0        0.0\n",
      "1    0.0      0.0   0.0    0.0      1.0  0.0        0.0\n",
      "2    0.0      0.0   0.0    0.0      1.0  0.0        0.0\n",
      "3    0.0      0.0   0.0    0.0      1.0  0.0        0.0\n",
      "4    0.0      0.0   0.0    0.0      0.0  0.0        1.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Choose an appropriate word and find the words that are the most similar to it in one of the pre-processed data files.  (30 points)",
   "id": "266fe5f034d63b4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T19:07:16.402275Z",
     "start_time": "2024-10-10T19:07:10.131468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# make a list of movie review documents\n",
    "# Load the preprocessed data\n",
    "data = (pd.read_csv('../../../../data/text/combined_cleaned.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "chosen_word = 'happy'\n",
    "\n",
    "documents = [nltk.word_tokenize(row) for row in data['text']]\n",
    "model = Word2Vec(documents, min_count=5)\n",
    "similar_words = model.wv.most_similar(positive = [chosen_word],topn = 25)\n",
    "\n",
    "if similar_words:\n",
    "    print(f\"Words most similar to '{chosen_word}':\")\n",
    "    for word, score in similar_words:\n",
    "        print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "    # Save the results to a text file\n",
    "    with open(f'similar_words_bow_{chosen_word}.txt', 'w') as f:\n",
    "        f.write(f\"Words most similar to '{chosen_word}':\\n\")\n",
    "        for word, score in similar_words:\n",
    "            f.write(f\"{word}: {score:.4f}\\n\")\n"
   ],
   "id": "649ce6751a953fb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'happy':\n",
      "thankful: 0.6261\n",
      "birthday: 0.6186\n",
      "holiday: 0.6133\n",
      "cheer: 0.6031\n",
      "thanksgiving: 0.5880\n",
      "bless: 0.5746\n",
      "excite: 0.5571\n",
      "friends: 0.5504\n",
      "busy: 0.5363\n",
      "wonderful: 0.5347\n",
      "invigorated: 0.5341\n",
      "lovely: 0.5316\n",
      "joy: 0.5316\n",
      "stress: 0.5315\n",
      "pleasant: 0.5294\n",
      "thebodyshopuk: 0.5283\n",
      "abruptly: 0.5261\n",
      "sick: 0.5205\n",
      "sweet: 0.5193\n",
      "dinner: 0.5095\n",
      "hope: 0.5093\n",
      "comfortable: 0.5039\n",
      "excellent: 0.4995\n",
      "celebrate: 0.4974\n",
      "present: 0.4961\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6471937777b9ce65"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
