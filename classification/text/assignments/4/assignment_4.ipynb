{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b02acbebf12ba9e",
   "metadata": {},
   "source": [
    "# Data Representations\n",
    "Author: Alikhan Semembayev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52486a8b8b66980e",
   "metadata": {},
   "source": [
    "## 1. Perform necessary data preprocessing, e.g. removing punctuation and stop words, stemming, lemmatizing. You may use the outputs from previous weekly assignments. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import demoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace emojis\n",
    "    text = demoji.replace(text)\n",
    "    \n",
    "    # Remove smart quotes and dashes\n",
    "    text = text.replace(\"“\", \"\\\"\").replace(\"”\",\"\\\"\").replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    # print(words)\n",
    "    \n",
    "    # Spelling correction + replace all t with not\n",
    "    words = ['not' if word == 't' else spell(word) for word in words]\n",
    "\n",
    "    # Remove stop words and non-alphabetic tokens and punctuation\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words or word in ['not', 'no']]\n",
    "\n",
    "    # POS tagging and Lemmatization\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    tag_map = defaultdict(lambda: \"n\")\n",
    "    tag_map[\"N\"] = \"n\"\n",
    "    tag_map[\"V\"] = \"v\"\n",
    "    tag_map[\"J\"] = \"a\"\n",
    "    tag_map[\"R\"] = \"r\"\n",
    "    \n",
    "    words = [lemmatizer.lemmatize(word, pos=tag_map[tag[0]]) for word, tag in tagged_words]\n",
    "\n",
    "    # Return cleaned words as a single string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff228db15ccb0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('../../data/text/combined_raw.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "for row in data.values:\n",
    "    row[0] = clean_text(row[0])\n",
    "\n",
    "data.to_csv('../../data/text/combined_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f19f6b5d8db59",
   "metadata": {},
   "source": [
    "## 2. Count BoW on pre-processed data.  (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e38b5d68f8f3c31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T20:36:06.821100Z",
     "start_time": "2024-10-08T20:36:06.545177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word  Count\n",
      "0  freshwater     40\n",
      "1        fish    605\n",
      "2       drink    379\n",
      "3       water    970\n",
      "4        skin    124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "data = (pd.read_csv('../../../../data/text/combined_cleaned.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "def count_bow(texts):\n",
    "    bow = Counter()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        bow.update(words)\n",
    "    return bow\n",
    "\n",
    "# Apply BoW counting\n",
    "bow_counts = count_bow(data['text'])\n",
    "\n",
    "# Convert the BoW count to a DataFrame\n",
    "bow_df = pd.DataFrame(list(bow_counts.items()), columns=['Word', 'Count'])\n",
    "\n",
    "# Save the result to a CSV file\n",
    "bow_df.to_csv('bow_output.csv', index=False)\n",
    "\n",
    "print(bow_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1c7420d7decc33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:46:24.477730Z",
     "start_time": "2024-10-10T18:46:24.431952Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('bow_output.csv'))\n",
    "\n",
    "data.sort_values(\"Count\", ascending=False, inplace=True)\n",
    "data = data.head(10)\n",
    "data.to_csv('bow_output_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eca627c7584f33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:46:28.847642Z",
     "start_time": "2024-10-10T18:46:26.844265Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "data = (pd.read_csv('bow_output_10.csv'))\n",
    "main = (pd.read_csv('../../../../data/text/combined_cleaned.csv'))\n",
    "main = main.dropna(how='any')\n",
    "main = main.head(1000)\n",
    "\n",
    "X = [] \n",
    "for row in main['text']: \n",
    "    vector = []\n",
    "    for word in data[\"Word\"]: \n",
    "        if word in nltk.word_tokenize(row): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bc42cbd715d23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:46:29.984356Z",
     "start_time": "2024-10-10T18:46:29.967996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW vectors saved to 'bow_vectors.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Convert X to a pandas DataFrame to save as CSV\n",
    "X_df = pd.DataFrame(X, columns=data[\"Word\"])\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "X_df.to_csv('bow_vectors.csv', index=False)\n",
    "\n",
    "print(\"BoW vectors saved to 'bow_vectors.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07fb10bbdac843",
   "metadata": {},
   "source": [
    "## 3. Compute TF-IDF vectors on pre-processed data.  (20 points)\n",
    "\n",
    "Due to the very large number of unique words, the output of the result has been changed. The second column of the table contains a sheet with TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2ff4a5aa75c65f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:49:37.554159Z",
     "start_time": "2024-10-10T18:49:37.544358Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Term Frequency (TF)\n",
    "def compute_tf(text):\n",
    "    word_counts = Counter(text.split())\n",
    "    total_words = len(text.split())\n",
    "    tf = {word: count / total_words for word, count in word_counts.items()}\n",
    "    return tf\n",
    "\n",
    "# Inverse Document Frequency (IDF)\n",
    "def compute_idf(texts):\n",
    "    N = len(texts)\n",
    "    idf = {}\n",
    "    for text in texts:\n",
    "        for word in set(text.split()):\n",
    "            idf[word] = idf.get(word, 0) + 1\n",
    "    for word, doc_count in idf.items():\n",
    "        idf[word] = math.log(N / (doc_count + 1))  # Add 1 to avoid division by zero\n",
    "    return idf\n",
    "\n",
    "# Compute TF-IDF for each document\n",
    "def compute_tfidf(texts):\n",
    "    idf = compute_idf(texts)\n",
    "    tfidf = []\n",
    "    for idx, text in enumerate(texts):\n",
    "        tf = compute_tf(text)\n",
    "        tfidf_doc = [(word, tf[word] * idf[word]) for word in tf]  # Store word and its TF-IDF score as tuple\n",
    "        tfidf.append({'Document': idx + 1, 'TF-IDF': tfidf_doc})  # Use index as document ID\n",
    "    return tfidf\n",
    "\n",
    "# Apply TF-IDF computation to the clean text\n",
    "tfidf_alternative = compute_tfidf(data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc5b4a1c14879cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T22:17:57.545943Z",
     "start_time": "2024-10-08T22:17:55.004480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Document                                             TF-IDF\n",
      "0         1  [(freshwater, 0.5148123472156599), (fish, 0.71...\n",
      "1         2  [(think, 0.23834696944350164), (everyone, 0.51...\n",
      "2         3  [(agree, 0.43660816089717763), (google, 1.0786...\n",
      "3         4  [(thats, 0.42304351063585904), (funny, 0.36702...\n",
      "4         5  [(oh, 0.2626823714954244), (yeah, 0.2042693288...\n"
     ]
    }
   ],
   "source": [
    "# Convert TF-IDF results to a DataFrame for easier inspection\n",
    "# Each document will be a row, and the TF-IDF scores will be a list of (word, score) tuples\n",
    "tfidf_df_alternative = pd.DataFrame(tfidf_alternative)\n",
    "\n",
    "# Save the revised TF-IDF table to a CSV file\n",
    "tfidf_df_alternative.to_csv('tfidf_output.csv', index=False)\n",
    "\n",
    "# Print the first few rows of the TF-IDF table\n",
    "print(tfidf_df_alternative.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f33405a51d33e88",
   "metadata": {},
   "source": [
    "## 4. Perform integer encoding and one-hot encoding on one of the pre-processed data files and save the output to a txt file.  (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "335405fe11a8b05f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T19:42:10.810362Z",
     "start_time": "2024-10-08T19:42:10.495044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emotion  emotion_encoded\n",
      "0      happy                3\n",
      "1    neutral                4\n",
      "2    neutral                4\n",
      "3    neutral                4\n",
      "4  surprised                6\n",
      "   angry  disgust  fear  happy  neutral  sad  surprised\n",
      "0    0.0      0.0   0.0    1.0      0.0  0.0        0.0\n",
      "1    0.0      0.0   0.0    0.0      1.0  0.0        0.0\n",
      "2    0.0      0.0   0.0    0.0      1.0  0.0        0.0\n",
      "3    0.0      0.0   0.0    0.0      1.0  0.0        0.0\n",
      "4    0.0      0.0   0.0    0.0      0.0  0.0        1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Integer encoding of the emotion labels\n",
    "data['emotion_encoded'] = label_encoder.fit_transform(data['emotion'])\n",
    "\n",
    "# Initialize one-hot encoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-hot encoding of the integer encoded labels\n",
    "emotion_onehot = onehot_encoder.fit_transform(data[['emotion_encoded']])\n",
    "\n",
    "# Save integer encoded and one-hot encoded labels to a file\n",
    "encoded_df = pd.DataFrame(emotion_onehot, columns=label_encoder.classes_)\n",
    "encoded_df.to_csv('emotion_onehot_encoded.txt', index=False)\n",
    "\n",
    "# Save integer encoding as well\n",
    "data[['emotion', 'emotion_encoded']].to_csv('emotion_integer_encoded.txt', index=False)\n",
    "\n",
    "# Display encoded data\n",
    "print(data[['emotion', 'emotion_encoded']].head())\n",
    "print(encoded_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fe5f034d63b4f",
   "metadata": {},
   "source": [
    "## 5. Choose an appropriate word and find the words that are the most similar to it in one of the pre-processed data files.  (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649ce6751a953fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T19:07:16.402275Z",
     "start_time": "2024-10-10T19:07:10.131468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'happy':\n",
      "thankful: 0.6261\n",
      "birthday: 0.6186\n",
      "holiday: 0.6133\n",
      "cheer: 0.6031\n",
      "thanksgiving: 0.5880\n",
      "bless: 0.5746\n",
      "excite: 0.5571\n",
      "friends: 0.5504\n",
      "busy: 0.5363\n",
      "wonderful: 0.5347\n",
      "invigorated: 0.5341\n",
      "lovely: 0.5316\n",
      "joy: 0.5316\n",
      "stress: 0.5315\n",
      "pleasant: 0.5294\n",
      "thebodyshopuk: 0.5283\n",
      "abruptly: 0.5261\n",
      "sick: 0.5205\n",
      "sweet: 0.5193\n",
      "dinner: 0.5095\n",
      "hope: 0.5093\n",
      "comfortable: 0.5039\n",
      "excellent: 0.4995\n",
      "celebrate: 0.4974\n",
      "present: 0.4961\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# make a list of movie review documents\n",
    "# Load the preprocessed data\n",
    "data = (pd.read_csv('../../../../data/text/combined_cleaned.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "chosen_word = 'happy'\n",
    "\n",
    "documents = [nltk.word_tokenize(row) for row in data['text']]\n",
    "model = Word2Vec(documents, min_count=5)\n",
    "similar_words = model.wv.most_similar(positive = [chosen_word],topn = 25)\n",
    "\n",
    "if similar_words:\n",
    "    print(f\"Words most similar to '{chosen_word}':\")\n",
    "    for word, score in similar_words:\n",
    "        print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "    # Save the results to a text file\n",
    "    with open(f'similar_words_bow_{chosen_word}.txt', 'w') as f:\n",
    "        f.write(f\"Words most similar to '{chosen_word}':\\n\")\n",
    "        for word, score in similar_words:\n",
    "            f.write(f\"{word}: {score:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471937777b9ce65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
