{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74992f0e642ead71",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Author: Alikhan Semembayev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed38f8f2f56948",
   "metadata": {},
   "source": [
    "## 1. Perform necessary data preprocessing, e.g. removing punctuation and stop words, stemming, lemmatizing. You may use the outputs from previous weekly assignments. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import demoji\n",
    "import svgling\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from autocorrect import Speller\n",
    "import re\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "email_re = r\"\\b[A-Za-z]+@\\S+\\b\"\n",
    "ssn_re = r\"\\b[0-9]{3}-[0-9]{2}-[0-9]{4}\\b\"\n",
    "ip_re = r\"\\b\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}\\b\"\n",
    "\n",
    "street_number_re = r\"^\\d{1,}\"\n",
    "street_name_re = r\"[a-zA-Z0-9\\s]+,?\"\n",
    "city_name_re = r\" [a-zA-Z]+(\\,)?\"\n",
    "state_abbrev_re = r\" [A-Z]{2}\"\n",
    "postal_code_re = r\" [0-9]{5}$\"\n",
    "address_pattern_re = r\"\" + street_number_re + street_name_re + city_name_re + state_abbrev_re + postal_code_re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace emojis\n",
    "    text = demoji.replace(text)\n",
    "\n",
    "    # Remove smart quotes and dashes\n",
    "    text = text.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    # print(words)\n",
    "\n",
    "    # Spelling correction + replace all t with not\n",
    "    words = ['not' if word == 't' else (\n",
    "        'ADDRESS' if re.match(address_pattern_re, word)\n",
    "        else (\n",
    "            'EMAIL' if re.match(email_re, word)\n",
    "            else (\n",
    "                'SSN' if re.match(ssn_re, word)\n",
    "                else (\n",
    "                    'IP' if re.match(ip_re, word)\n",
    "                    else spell(word)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ) for word in words]\n",
    "\n",
    "    # Remove stop words and non-alphabetic tokens and punctuation\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words or word in ['not', 'no']]\n",
    "\n",
    "    # POS tagging and Lemmatization\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    tag_map = defaultdict(lambda: \"n\")\n",
    "    tag_map[\"N\"] = \"n\"\n",
    "    tag_map[\"V\"] = \"v\"\n",
    "    tag_map[\"J\"] = \"a\"\n",
    "    tag_map[\"R\"] = \"r\"\n",
    "\n",
    "    words = [lemmatizer.lemmatize(word, pos=tag_map[tag[0]]) for word, tag in tagged_words]\n",
    "\n",
    "    # Return cleaned words as a single string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd317e7000b2d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('../../../../data/text/combined_raw.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "for row in data.values:\n",
    "    row[0] = clean_text(row[0])\n",
    "\n",
    "data.to_csv('../../../../data/text/combined_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989257dd741140f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T01:23:05.566218Z",
     "start_time": "2024-10-29T01:23:05.335124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    emotion\n",
      "0  freshwater fish drink water skin via osmosis s...      happy\n",
      "1  think everyone must use daily become grained e...    neutral\n",
      "2  agree google headquarters mountain view califo...    neutral\n",
      "3  thats funny current ceo sunday ficha didnt kno...    neutral\n",
      "4  oh yeah not know either also want go google al...  surprised\n",
      "5                                                say  surprised\n",
      "6        yeah apparently lol instead hire people row      happy\n",
      "7  thats funny guess imaginative leave huge tech ...  surprised\n",
      "8  yeah exactly sure cheap one thing bet not expl...  surprised\n",
      "9  remember hearing immortality waste jellyfish h...    neutral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('../../../../data/text/combined_cleaned.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2143032363475",
   "metadata": {},
   "source": [
    "## 2. For the binary classification problem you came up last week, set up a MLP to solve it.  (50 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66abd9b6d78bcce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:27:32.807518Z",
     "start_time": "2024-10-31T01:27:21.481105Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Custom Dataset Class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.texts[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Define the Model\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim * max_length, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        embedded = embedded.view(embedded.size(0), -1)  # Flatten the embeddings\n",
    "        x = torch.relu(self.fc1(embedded))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2a6017b6be62fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:39:25.509152Z",
     "start_time": "2024-10-31T01:39:25.247062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    emotion\n",
      "0  freshwater fish drink water skin via osmosis s...      happy\n",
      "1  think everyone must use daily become grained e...    neutral\n",
      "2  agree google headquarters mountain view califo...    neutral\n",
      "3  thats funny current ceo sunday ficha didnt kno...    neutral\n",
      "4  oh yeah not know either also want go google al...  surprised\n",
      "5                                                say  surprised\n",
      "6        yeah apparently lol instead hire people row      happy\n",
      "7  thats funny guess imaginative leave huge tech ...  surprised\n",
      "8  yeah exactly sure cheap one thing bet not expl...  surprised\n",
      "9  remember hearing immortality waste jellyfish h...    neutral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = (pd.read_csv('../../../../data/text/combined_cleaned.csv'))\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffcc66bd4b8b3f0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:41:59.778799Z",
     "start_time": "2024-10-31T01:41:36.339904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['emotion'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 50\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encodings['input_ids'].tolist()\n",
    "\n",
    "# Convert texts to token IDs\n",
    "data['input_ids'] = tokenize_texts(data['text'])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['input_ids'].tolist(), data['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e69199ecc392f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:42:04.250420Z",
     "start_time": "2024-10-31T01:42:04.239124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset = EmotionDataset(X_train, y_train)\n",
    "test_dataset = EmotionDataset(X_test, y_test)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70aee8da82f3221",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:45:33.272032Z",
     "start_time": "2024-10-31T01:43:37.772028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/10, Loss: 1.737734079360962\n",
      "Epoch 2/10, Loss: 1.3603670597076416\n",
      "Epoch 3/10, Loss: 1.4432244300842285\n",
      "Epoch 4/10, Loss: 1.255486011505127\n",
      "Epoch 5/10, Loss: 1.4124947786331177\n",
      "Epoch 6/10, Loss: 1.295527458190918\n",
      "Epoch 7/10, Loss: 1.4154021739959717\n",
      "Epoch 8/10, Loss: 1.5953384637832642\n",
      "Epoch 9/10, Loss: 1.2087442874908447\n",
      "Epoch 10/10, Loss: 1.2303894758224487\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.38      0.20      0.26      1518\n",
      "     disgust       0.00      0.00      0.00       462\n",
      "        fear       0.45      0.25      0.32      1631\n",
      "       happy       0.46      0.42      0.44      8580\n",
      "     neutral       0.45      0.54      0.49      8571\n",
      "         sad       0.29      0.21      0.24      1864\n",
      "   surprised       0.45      0.55      0.49      6850\n",
      "\n",
      "    accuracy                           0.44     29476\n",
      "   macro avg       0.35      0.31      0.32     29476\n",
      "weighted avg       0.43      0.44      0.43     29476\n",
      "\n",
      "Accuracy: 0.4440561813000407\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "vocab_size = tokenizer.vocab_size  # Use tokenizer's vocab size\n",
    "embed_dim = 50\n",
    "model = EmotionClassifier(vocab_size, embed_dim, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Move predictions and labels back to CPU for evaluation\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e57d12f6c15c3b",
   "metadata": {},
   "source": [
    "## 3. Try to improve performance by modifying hyperparameters. (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5abf8fd19076ca0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T02:11:56.141587Z",
     "start_time": "2024-10-31T02:09:01.854236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/15, Loss: 1.5425374507904053\n",
      "Epoch 2/15, Loss: 1.3963515758514404\n",
      "Epoch 3/15, Loss: 1.3071774244308472\n",
      "Epoch 4/15, Loss: 1.2933820486068726\n",
      "Epoch 5/15, Loss: 1.2624475955963135\n",
      "Epoch 6/15, Loss: 1.4152458906173706\n",
      "Epoch 7/15, Loss: 1.472933292388916\n",
      "Epoch 8/15, Loss: 1.231341004371643\n",
      "Epoch 9/15, Loss: 1.1810821294784546\n",
      "Epoch 10/15, Loss: 0.9682900309562683\n",
      "Epoch 11/15, Loss: 0.9055383205413818\n",
      "Epoch 12/15, Loss: 1.3185577392578125\n",
      "Epoch 13/15, Loss: 1.6052885055541992\n",
      "Epoch 14/15, Loss: 1.0228430032730103\n",
      "Epoch 15/15, Loss: 1.3833003044128418\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.45      0.06      0.11      1518\n",
      "     disgust       0.00      0.00      0.00       462\n",
      "        fear       0.37      0.47      0.41      1631\n",
      "       happy       0.49      0.39      0.43      8580\n",
      "     neutral       0.47      0.62      0.53      8571\n",
      "         sad       0.33      0.20      0.25      1864\n",
      "   surprised       0.46      0.53      0.49      6850\n",
      "\n",
      "    accuracy                           0.46     29476\n",
      "   macro avg       0.37      0.32      0.32     29476\n",
      "weighted avg       0.45      0.46      0.44     29476\n",
      "\n",
      "Accuracy: 0.45908535757904734\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "vocab_size = tokenizer.vocab_size  # Use tokenizer's vocab size\n",
    "embed_dim = 50\n",
    "model = EmotionClassifier(vocab_size, embed_dim, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Move predictions and labels back to CPU for evaluation\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683703ea86888ccd",
   "metadata": {},
   "source": [
    "## 4. Summarize what you have learned and discovered from Task 1-3 as well as the tasks you completed last week.(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847e8bab0519cef",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "1. Preprocessing is very important stage of development. Using stemming and lemmatization, along with removing stop words, helps improve text data representation. It decreases the size of dataset by removing unnecessary information, and optimizes it for training.\n",
    "2. Using an initial MLP setup, we achieved an accuracy of 44.4% on the binary classification problem. This MLP is very simple and has only 3 linear layers.\n",
    "3. After tuning hyperparameters such as learning rate(0.0004=>0.0001) and number of epochs(10=>15), we noticed an improvement in accuracy to 45.9%. I believe that increasing the number of epochs will improve accuracy even more.\n",
    "4. Binary classification has better accuracy, because of fewer labels(possible outputs). Multicalss problem is more difficult and requires bigger model and more time. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
